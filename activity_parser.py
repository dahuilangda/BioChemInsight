import os
import sys
from utils.pdf_utils import pdf_to_markdown, dots_ocr
from utils.llm_utils import content_to_dict, configure_genai
from utils.file_utils import write_json_file, read_text_file
# from constants import GEMINI_API_KEY, GEMINI_MODEL_NAME

try:
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
    if os.path.basename(SCRIPT_DIR) != '' and os.path.exists(os.path.join(SCRIPT_DIR, '..', 'constants.py')):
         sys.path.insert(0, os.path.join(SCRIPT_DIR, '..'))
         import constants
         sys.path.pop(0)
    else:
        import constants
except ImportError:
    print("Error: constants.py not found. Please ensure it's in the same directory, parent directory, or your PYTHONPATH.")
    sys.exit(1)

GEMINI_API_KEY = getattr(constants, 'GEMINI_API_KEY', None)
GEMINI_MODEL_NAME = getattr(constants, 'GEMINI_MODEL_NAME', 'gemini-2.0-flash')

def extract_activity_data(pdf_file, assay_page_start, assay_page_end, assay_name,
                          compound_id_list, output_dir, pages_per_chunk=3, lang='en', ocr_engine='paddleocr', ocr_server='http://localhost:8001', progress_callback=None):
    """
    æ ¹æ®PDFæŒ‡å®šé¡µç èŒƒå›´è§£ææ•°æ®ï¼š
    
    1. ä½¿ç”¨ pdf_to_markdown å°† PDF ä¸­ assay_page_start åˆ° assay_page_end é¡µè½¬æ¢ä¸º Markdown æ ¼å¼ï¼Œ
       è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºé¡µç ï¼Œå€¼ä¸ºå¯¹åº”é¡µé¢å†…å®¹ã€‚
    2. æ ¹æ®å‚æ•° pages_per_chunkï¼Œå°†å¤šä¸ªè¿ç»­é¡µé¢çš„ Markdown å†…å®¹ç»„åˆä¸ºä¸€ä¸ª chunkï¼Œ
       æ¯ä¸ª chunk å†…éƒ¨çš„å†…å®¹é€šè¿‡é¡µç ä¿¡æ¯åˆ†éš”ï¼Œä¿æŒåŸæœ‰é¡µé¢ç»“æ„ã€‚
    3. é’ˆå¯¹æ¯ä¸ª chunk è°ƒç”¨ content_to_dict è¿›è¡Œæ•°æ®æå–ï¼Œå¹¶åˆå¹¶å„chunkçš„ç»“æœã€‚
    4. æœ€åå°†åˆå¹¶åçš„ç»“æœä¿å­˜ä¸º JSON æ–‡ä»¶ï¼Œå¹¶è¿”å› assay_dictã€‚
    
    å‚æ•°:
      pdf_file (str): PDF æ–‡ä»¶è·¯å¾„ã€‚
      assay_page_start (int): èµ·å§‹é¡µç ã€‚
      assay_page_end (int): ç»“æŸé¡µç ã€‚
      assay_name (str): æµ‹å®šåç§°ã€‚
      compound_id_list (list): åŒ–åˆç‰©IDåˆ—è¡¨ï¼Œç”¨äºæç¤ºã€‚
      output_dir (str): è¾“å‡ºç›®å½•ã€‚
      pages_per_chunk (int): æ¯ä¸ª chunk åŒ…å«çš„é¡µæ•°ã€‚
      lang (str): PDFè½¬æ¢æ—¶ä½¿ç”¨çš„è¯­è¨€ï¼Œé»˜è®¤ä¸ºè‹±æ–‡ã€‚
    """

    assay_dict = {}
    content_list = []
    
    total_pages = assay_page_end - assay_page_start + 1
    
    if progress_callback:
        progress_callback(f"ğŸ§ª å¼€å§‹å¤„ç†æ´»æ€§æ•°æ®é¡µé¢ {assay_page_start}-{assay_page_end} (å…± {total_pages} é¡µ)")

    if ocr_engine == 'paddleocr':
        # ä½¿ç”¨ paddleocr è§£æ PDF
        if progress_callback:
            progress_callback(f"ğŸ“– ä½¿ç”¨ PaddleOCR å¤„ç†ç¬¬ {assay_page_start} é¡µåˆ°ç¬¬ {assay_page_end} é¡µ")
        print(f"Processing pages with PaddleOCR...")
        # å°†æŒ‡å®šé¡µç çš„å†…å®¹è½¬ä¸º Markdownï¼Œå‡è®¾è¿”å›ä¸€ä¸ªå­—å…¸ {é¡µç : markdownæ–‡æœ¬}
        assay_md_file = pdf_to_markdown(pdf_file, output_dir, page_start=assay_page_start,
                                                page_end=assay_page_end, lang=lang)
        
        content = read_text_file(assay_md_file)
        content_pages = [line.strip() for line in content.split('\n\n-#-#-#-#-\n\n') if line.strip()]
        content_list.extend(content_pages)

    elif ocr_engine == 'dots_ocr':
        # ä½¿ç”¨ dots_ocr è§£æ PDF
        for aps in range(assay_page_start, assay_page_end + 1):
            current_page_idx = aps - assay_page_start + 1
            if progress_callback:
                progress_callback(f"ğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {current_page_idx} é¡µï¼Œå…± {total_pages} é¡µ (é¡µé¢ {aps})")
            # å°†æŒ‡å®šé¡µç çš„å†…å®¹è½¬ä¸º Markdownï¼Œå‡è®¾è¿”å›ä¸€ä¸ªåˆ—è¡¨ [markdownæ–‡ä»¶è·¯å¾„]
            assay_md_files = dots_ocr(pdf_file, output_dir, page_start=aps, page_end=aps)
            assay_md_file = assay_md_files[0]
            content = read_text_file(assay_md_file)
            content_list.append(content)
    
    chunks = []
    for i in range(0, len(content_list), pages_per_chunk):
        group_pages = content_list[i:i + pages_per_chunk]
        chunk_text = "\n\n".join(group_pages)
        chunks.append(chunk_text)

    if progress_callback:
        progress_callback(f"ğŸ“Š å°† {total_pages} é¡µå†…å®¹åˆ†ä¸º {len(chunks)} ä¸ªæ•°æ®å—è¿›è¡Œå¤„ç†")
    print(f"Total {len(chunks)} chunks to process.")
        
    # é’ˆå¯¹æ¯ä¸ª chunk è°ƒç”¨ content_to_dict è¿›è¡Œæå–
    for idx, chunk in enumerate(chunks, 1):
        if progress_callback:
            progress_callback(f"ğŸ” æ­£åœ¨åˆ†æç¬¬ {idx} ä¸ªæ•°æ®å—ï¼Œå…± {len(chunks)} ä¸ª")
        print(f"Processing chunk {idx}/{len(chunks)}...")
        print('Chunk content preview:', chunk[:1000])  # Preview first 1000 characters
        chunk_assay_dict = content_to_dict(chunk, assay_name, compound_id_list=compound_id_list)
        if chunk_assay_dict:
            assay_dict.update(chunk_assay_dict)
        else:
            print(f"Warning: Chunk {idx} returned empty results.")

    print(f"Extracted total assay data entries: {len(assay_dict)}")
    
    # ä¿å­˜æå–ç»“æœè‡³ JSON æ–‡ä»¶
    output_json = f'{output_dir}/assay_data.json'
    print(f"Saving assay data to {output_json}")
    write_json_file(output_json, assay_dict)

    return assay_dict